{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask dataframes on HDFS\n",
    "\n",
    "To use Dask dataframes in parallel across an HDFS cluster to read CSV data. We can coordinate these computations with [distributed](http://distributed.dask.org/en/latest/) and dask.dataframe.\n",
    "\n",
    "As Spark, Dask can work in cluster mode. On INSA cluster you can use the dask module [dask_jobqueue](https://jobqueue.dask.org/en/latest/) to launch a Dask cluster with the job manager SLURM.\n",
    "\n",
    "```python\n",
    "from dask_jobqueue import SLURMCluster\n",
    "\n",
    "cluster = SLURMCluster(processes=4,\n",
    "                       threads=2,\n",
    "                       memory=\"16GB\",\n",
    "                       walltime=\"01:00:00\")\n",
    "```\n",
    "\n",
    "The cluster generates a traditional job script and submits that an appropriate number of times to the job queue. You can see the job script that it will generate as follows:\n",
    "\n",
    "```\n",
    "print(cluster.job_script())\n",
    "```\n",
    "\n",
    "Access to the cluster using following lines:\n",
    "\n",
    "```\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client\n",
    "client = Client(cluster)\n",
    "```\n",
    "\n",
    "`nyc2014` is a dask.dataframe objects which present a subset of the Pandas API to the user, but farm out all of the work to the many Pandas dataframes they control across the network.\n",
    "\n",
    "```python\n",
    "nyc2014 = dd.read_csv('/opt/datasets/nyc-data/2014/yellow*.csv',\n",
    "parse_dates=['pickup_datetime', 'dropoff_datetime'],\n",
    "skipinitialspace=True)\n",
    "nyc2014 = c.persist(nyc2014)\n",
    "progress(nyc2014)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise \n",
    "\n",
    "- Display head of the dataframe\n",
    "- Display number of rows of this dataframe.\n",
    "- Compute the total number of passengers.\n",
    "- Count occurrences in the payment_type column both for the full dataset, and filtered by zero tip (tip_amount == 0).\n",
    "- Create a new column, tip_fraction\n",
    "- Plot the average of the new column tip_fraction grouped by day of week.\n",
    "- Plot the average of the new column tip_fraction grouped by hour of day.\n",
    "\n",
    "[Dask dataframe documentation](http://docs.dask.org/en/latest/dataframe.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Missing required dependencies ['numpy']\n\nDask dataframe requirements are not installed.\n\nPlease either conda or pip install as follows:\n\n  conda install dask                     # either conda install\n  pip install dask[dataframe] --upgrade  # or pip install",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/opt/miniconda3/envs/big-data/lib/python3.6/site-packages/dask/dataframe/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     from .core import (DataFrame, Series, Index, _Frame, map_partitions,\n\u001b[0m\u001b[1;32m      5\u001b[0m                        repartition, to_datetime, to_timedelta)\n",
      "\u001b[0;32m/opt/miniconda3/envs/big-data/lib/python3.6/site-packages/dask/dataframe/core.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtoolz\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmerge\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartition_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/big-data/lib/python3.6/site-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     raise ImportError(\n\u001b[0;32m---> 19\u001b[0;31m         \"Missing required dependencies {0}\".format(missing_dependencies))\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mhard_dependencies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdependency\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing_dependencies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: Missing required dependencies ['numpy']",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-53aefc54012d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataframe\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdistributed\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdask_jobqueue\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSLURMCluster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/big-data/lib/python3.6/site-packages/dask/dataframe/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m            \u001b[0;34m\"  conda install dask                     # either conda install\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m            \"  pip install dask[dataframe] --upgrade  # or pip install\")\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n\\n'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: Missing required dependencies ['numpy']\n\nDask dataframe requirements are not installed.\n\nPlease either conda or pip install as follows:\n\n  conda install dask                     # either conda install\n  pip install dask[dataframe] --upgrade  # or pip install"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "from distributed import Client, progress\n",
    "import os\n",
    "\n",
    "from dask_jobqueue import SLURMCluster\n",
    "\n",
    "cluster = SLURMCluster(processes=4,\n",
    "                       threads=4,\n",
    "                       memory=\"16GB\",\n",
    "                       walltime=\"01:00:00\")\n",
    "\n",
    "c = Client(cluster)\n",
    "c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "408dec74eba94d5c8abaae8e2bed4f1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nyc2014 = dd.read_csv('file:///opt/datasets/nyc-data/2014/yellow*.csv',\n",
    "parse_dates=['pickup_datetime', 'dropoff_datetime'],\n",
    "skipinitialspace=True)\n",
    "nyc2014 = c.persist(nyc2014)\n",
    "progress(nyc2014)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (big-data)",
   "language": "python",
   "name": "big-data"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
