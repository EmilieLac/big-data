{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Apache Spark\n",
    "\n",
    "![Logo](images/apache_spark_logo.png)\n",
    "\n",
    "- [Apache Spark](https://spark.apache.org) was first released in 2014. \n",
    "- It was originally developed by [Matei Zaharia](http://people.csail.mit.edu/matei) as a class project, and later a PhD dissertation, at University of California, Berkeley.\n",
    "- Spark is written in [Scala](https://www.scala-lang.org)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Apache Spark is a fast and general-purpose cluster computing system. \n",
    "- It provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs.\n",
    "- Spark can manage \"big data\" collections with a small set of high-level primitives like `map`, `filter`, `groupby`, and `join`.  With these common patterns we can often handle computations that are more complex than map, but are still structured.\n",
    "- It also supports a rich set of higher-level tools including [Spark SQL](https://spark.apache.org/docs/latest/sql-programming-guide.html) for SQL and structured data processing, [MLlib](https://spark.apache.org/docs/latest/ml-guide.html) for machine learning, [GraphX](https://spark.apache.org/docs/latest/graphx-programming-guide.html) for graph processing, and Spark Streaming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Resilient distributed datasets\n",
    "\n",
    "- The fundamental abstraction of Apache Spark is a read-only, parallel, distributed, fault-tolerent collection called a resilient distributed datasets (RDD).\n",
    "- RDDs behave a bit like Python collections (e.g. lists).\n",
    "- When working with Apache Spark we iteratively apply functions to every item of these collections in parallel to produce *new* RDDs.\n",
    "- The data is distributed across nodes in a cluster of computers.\n",
    "- Functions implemented in Spark can work in parallel across elements of the collection.\n",
    "- The  Spark framework allocates data and processing to different nodes, without any intervention from the programmer.\n",
    "- RDDs automatically rebuilt on machine failure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lifecycle of a Spark Program\n",
    "1. Create some input RDDs from external data or parallelize a collection in your driver program.\n",
    "2. Lazily transform them to define new RDDs using transformations like `filter()` or `map()`\n",
    "3. Ask Spark to cache() any intermediate RDDs that will need to be reused.\n",
    "4. Launch actions such as count() and collect() to kick off a parallel computation, which is then optimized and executed by Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Operations on Distributed Data\n",
    "- Two types of operations: **transformations** and **actions**\n",
    "- Transformations are *lazy* (not computed immediately) \n",
    "- Transformations are executed when an action is run\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Transformations](https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations) (lazy)\n",
    "```spark\n",
    "map() flatMap()\n",
    "filter() \n",
    "mapPartitions() mapPartitionsWithIndex() \n",
    "sample()\n",
    "union() intersection() distinct()\n",
    "groupBy() groupByKey()\n",
    "reduceBy() reduceByKey()\n",
    "sortBy() sortByKey()\n",
    "join()\n",
    "cogroup()\n",
    "cartesian()\n",
    "pipe()\n",
    "coalesce()\n",
    "repartition()\n",
    "partitionBy()\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Actions](https://spark.apache.org/docs/latest/rdd-programming-guide.html#actions)\n",
    "\n",
    "```\n",
    "reduce()\n",
    "collect()\n",
    "count()\n",
    "first()\n",
    "take()\n",
    "takeSample()\n",
    "saveToCassandra()\n",
    "takeOrdered()\n",
    "saveAsTextFile()\n",
    "saveAsSequenceFile()\n",
    "saveAsObjectFile()\n",
    "countByKey()\n",
    "foreach()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PySpark\n",
    "\n",
    "\n",
    "PySpark uses Py4J that enables Python programs to dynamically access Java objects.\n",
    "\n",
    "![PySpark Internals](http://i.imgur.com/YlI8AqEl.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The `SparkContext` class\n",
    "\n",
    "- When working with Apache Spark we invoke methods on an object which is an instance of the `pyspark.SparkContext` context.\n",
    "\n",
    "- Typically, an instance of this object will be created automatically for you and assigned to the variable `sc`.\n",
    "\n",
    "- The `parallelize` method in `SparkContext` can be used to turn any ordinary Python collection into an RDD;\n",
    "    - normally we would create an RDD from a large file or an HBase table. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## First example\n",
    "\n",
    "PySpark isn't on sys.path by default, but that doesn't mean it can't be used as a regular library. You can address this by either symlinking pyspark into your site-packages, or adding pyspark to sys.path at runtime. [findspark](https://github.com/minrk/findspark) does the latter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import findspark, pyspark\n",
    "\n",
    "findspark.init(spark_home=\"/opt/spark-2.3.1-bin-hadoop2.7/\")\n",
    "\n",
    "sc = pyspark.SparkContext(master=\"local[8]\", appName=\"FirstExample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# If tou get an error run this cell with the command below commented out\n",
    "# and fix the path to spark and/or python in the cell above\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We have a spark context sc to use with a tiny local spark cluster with 2 nodes (will work just fine on a multicore machine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[8] appName=FirstExample>\n"
     ]
    }
   ],
   "source": [
    "print(sc) # it is like a Pool Processor executor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Create your first RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(list(range(8))) # create collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:184"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()  # Gather results back to local process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "square = rdd.map(lambda x: x ** 2) # Square each element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 4, 9, 16, 25, 36, 49]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "square.collect() # Square each element and collect results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Map-Reduce operation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import add\n",
    "rdd.map(lambda x: x ** 2).reduce(add) # reduce is an action!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 4, 6]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select only the even elements\n",
    "rdd.filter(lambda x: x % 2 == 0).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0),\n",
       " (0, 1),\n",
       " (0, 2),\n",
       " (0, 3),\n",
       " (0, 4),\n",
       " (0, 5),\n",
       " (0, 6),\n",
       " (0, 7),\n",
       " (1, 0),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (1, 3),\n",
       " (1, 4),\n",
       " (1, 5),\n",
       " (1, 6),\n",
       " (1, 7),\n",
       " (2, 0),\n",
       " (2, 1),\n",
       " (2, 2),\n",
       " (2, 3),\n",
       " (2, 4),\n",
       " (2, 5),\n",
       " (2, 6),\n",
       " (2, 7),\n",
       " (3, 0),\n",
       " (3, 1),\n",
       " (3, 2),\n",
       " (3, 3),\n",
       " (3, 4),\n",
       " (3, 5),\n",
       " (3, 6),\n",
       " (3, 7),\n",
       " (4, 0),\n",
       " (4, 1),\n",
       " (4, 2),\n",
       " (4, 3),\n",
       " (4, 4),\n",
       " (4, 5),\n",
       " (4, 6),\n",
       " (4, 7),\n",
       " (5, 0),\n",
       " (5, 1),\n",
       " (5, 2),\n",
       " (5, 3),\n",
       " (5, 4),\n",
       " (5, 5),\n",
       " (5, 6),\n",
       " (5, 7),\n",
       " (6, 0),\n",
       " (6, 1),\n",
       " (6, 2),\n",
       " (6, 3),\n",
       " (6, 4),\n",
       " (6, 5),\n",
       " (6, 6),\n",
       " (6, 7),\n",
       " (7, 0),\n",
       " (7, 1),\n",
       " (7, 2),\n",
       " (7, 3),\n",
       " (7, 4),\n",
       " (7, 5),\n",
       " (7, 6),\n",
       " (7, 7)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cartesian product of each pair of elements in two sequences \n",
    "# (or the same sequence in this case)\n",
    "rdd.cartesian(rdd).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0),\n",
       " (0, 1),\n",
       " (0, 2),\n",
       " (0, 3),\n",
       " (0, 4),\n",
       " (0, 5),\n",
       " (0, 6),\n",
       " (0, 7),\n",
       " (4, 0),\n",
       " (4, 1),\n",
       " (4, 2),\n",
       " (4, 3),\n",
       " (4, 4),\n",
       " (4, 5),\n",
       " (4, 6),\n",
       " (4, 7),\n",
       " (16, 0),\n",
       " (16, 1),\n",
       " (16, 2),\n",
       " (16, 3),\n",
       " (16, 4),\n",
       " (16, 5),\n",
       " (16, 6),\n",
       " (16, 7),\n",
       " (36, 0),\n",
       " (36, 1),\n",
       " (36, 2),\n",
       " (36, 3),\n",
       " (36, 4),\n",
       " (36, 5),\n",
       " (36, 6),\n",
       " (36, 7)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chain operations to construct more complex computations\n",
    "(rdd.map(lambda x: x ** 2)\n",
    "    .cartesian(rdd)\n",
    "    .filter(lambda tup: tup[0] % 2 == 0)\n",
    "    .collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Stop the local spark cluster\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise 9.1 Word-count in Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Write the sample text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from lorem import text\n",
    "with open('sample.txt','w') as f:\n",
    "    f.write(text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "- Create the rdd with `SparkContext.textFile method`\n",
    "- lower, remove dots and split using `rdd.flatMap`\n",
    "- use `rdd.map` to create the list of key/value pair (word, 1)\n",
    "- `rdd.reduceByKey` to get all occurences\n",
    "- `rdd.takeOrdered`to get sorted frequencies of words\n",
    "\n",
    "All documentation is available [here](https://spark.apache.org/docs/2.1.0/api/python/pyspark.html?highlight=textfile#pyspark.SparkContext) for textFile and [here](https://spark.apache.org/docs/2.1.0/api/python/pyspark.html?highlight=textfile#pyspark.RDD) for RDD. \n",
    "\n",
    "For a global overview see the Transformations section of the [programming guide](https://spark.apache.org/docs/latest/rdd-programming-guide.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SparkSession\n",
    "\n",
    "Since SPARK 2.0.0,  SparkSession provides a single point \n",
    "of entry to interact with Spark functionality and\n",
    "allows programming Spark with DataFrame and Dataset APIs. \n",
    "\n",
    "### $\\pi$ computation example\n",
    "\n",
    "- We can estimate an approximate value for $\\pi$ using the following Monte-Carlo method:\n",
    "\n",
    "1.    Inscribe a circle in a square\n",
    "2.    Randomly generate points in the square\n",
    "3.    Determine the number of points in the square that are also in the circle\n",
    "4.    Let $r$ be the number of points in the circle divided by the number of points in the square, then $\\pi \\approx 4 r$.\n",
    "    \n",
    "- Note that the more points generated, the better the approximation\n",
    "\n",
    "See [this tutorial](https://computing.llnl.gov/tutorials/parallel_comp/#ExamplesPI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi is roughly 3.142976\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from random import random\n",
    "from operator import add\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder.master(\"local[4]\")\n",
    "         .appName(\"PythonPi\")\n",
    "         .config(\"spark.executor.instances\", \"4\")\n",
    "         .getOrCreate())\n",
    "\n",
    "partitions = 8\n",
    "n = 1000000 * partitions\n",
    "\n",
    "def f(_):\n",
    "    x = random() * 2 - 1\n",
    "    y = random() * 2 - 1\n",
    "    return 1 if x ** 2 + y ** 2 <= 1 else 0\n",
    "\n",
    "count = spark.sparkContext.parallelize(range(1, n+1), partitions).map(f).reduce(add)\n",
    "print(\"Pi is roughly %f\" % (4.0 * count / n))\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise 9.2\n",
    "\n",
    "Using the same method than the PI computation example, compute the integral\n",
    "$$\n",
    "I = \\int_0^1 \\exp(-x^2) dx\n",
    "$$\n",
    "You can check your result with numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7468240713763741"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numpy evaluates solution using numeric computation. \n",
    "# It uses discrete values of the function\n",
    "import numpy as np\n",
    "x = np.linspace(0,1,1000)\n",
    "np.trapz(np.exp(-x*x),x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7468241328124271, 8.291413475940725e-15)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numpy and scipy evaluates solution using numeric computation. It uses discrete values\n",
    "# of the function\n",
    "import numpy as np\n",
    "from scipy.integrate import quad\n",
    "quad(lambda x: np.exp(-x*x), 0, 1)\n",
    "# note: the solution returned is complex "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Correlation between daily stock\n",
    "\n",
    "- Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/daily-stock/aet.json',\n",
       " '../data/daily-stock/afl.json',\n",
       " '../data/daily-stock/aig.json',\n",
       " '../data/daily-stock/al.json',\n",
       " '../data/daily-stock/amgn.json',\n",
       " '../data/daily-stock/avy.json',\n",
       " '../data/daily-stock/b.json',\n",
       " '../data/daily-stock/bwa.json',\n",
       " '../data/daily-stock/ge.json',\n",
       " '../data/daily-stock/hal.json',\n",
       " '../data/daily-stock/hp.json',\n",
       " '../data/daily-stock/hpq.json',\n",
       " '../data/daily-stock/ibm.json',\n",
       " '../data/daily-stock/jbl.json',\n",
       " '../data/daily-stock/jpm.json',\n",
       " '../data/daily-stock/luv.json',\n",
       " '../data/daily-stock/met.json',\n",
       " '../data/daily-stock/pcg.json',\n",
       " '../data/daily-stock/tgt.json',\n",
       " '../data/daily-stock/usb.json',\n",
       " '../data/daily-stock/xom.json']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob\n",
    "import os, json\n",
    "import pandas as pd\n",
    "\n",
    "filenames = sorted(glob(os.path.join('..','data','daily-stock', '*.json')))\n",
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished : aet.h5\n",
      "Finished : afl.h5\n",
      "Finished : aig.h5\n",
      "Finished : al.h5\n",
      "Finished : amgn.h5\n",
      "Finished : avy.h5\n",
      "Finished : b.h5\n",
      "Finished : bwa.h5\n",
      "Finished : ge.h5\n",
      "Finished : hal.h5\n",
      "Finished : hp.h5\n",
      "Finished : hpq.h5\n",
      "Finished : ibm.h5\n",
      "Finished : jbl.h5\n",
      "Finished : jpm.h5\n",
      "Finished : luv.h5\n",
      "Finished : met.h5\n",
      "Finished : pcg.h5\n",
      "Finished : tgt.h5\n",
      "Finished : usb.h5\n",
      "Finished : xom.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../data/daily-stock/aet.h5',\n",
       " '../data/daily-stock/afl.h5',\n",
       " '../data/daily-stock/aig.h5',\n",
       " '../data/daily-stock/al.h5',\n",
       " '../data/daily-stock/amgn.h5',\n",
       " '../data/daily-stock/avy.h5',\n",
       " '../data/daily-stock/b.h5',\n",
       " '../data/daily-stock/bwa.h5',\n",
       " '../data/daily-stock/ge.h5',\n",
       " '../data/daily-stock/hal.h5',\n",
       " '../data/daily-stock/hp.h5',\n",
       " '../data/daily-stock/hpq.h5',\n",
       " '../data/daily-stock/ibm.h5',\n",
       " '../data/daily-stock/jbl.h5',\n",
       " '../data/daily-stock/jpm.h5',\n",
       " '../data/daily-stock/luv.h5',\n",
       " '../data/daily-stock/met.h5',\n",
       " '../data/daily-stock/pcg.h5',\n",
       " '../data/daily-stock/tgt.h5',\n",
       " '../data/daily-stock/usb.h5',\n",
       " '../data/daily-stock/xom.h5']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for fn in filenames:\n",
    "    with open(fn) as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "        \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    out_filename = fn[:-5] + '.h5'\n",
    "    df.to_hdf(out_filename, '/data')\n",
    "    print(\"Finished : %s\" % out_filename.split(os.path.sep)[-1])\n",
    "\n",
    "filenames = sorted(glob(os.path.join('..','data', 'daily-stock', '*.h5')))  # ../data/json/*.json\n",
    "filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sequential code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.35 s, sys: 121 ms, total: 4.47 s\n",
      "Wall time: 1.72 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "series = []\n",
    "for fn in filenames:   # Simple map over filenames\n",
    "    series.append(pd.read_hdf(fn)['close'])\n",
    "\n",
    "results = []\n",
    "\n",
    "for a in series:    # Doubly nested loop over the same collection\n",
    "    for b in series:  \n",
    "        if not (a == b).all():     # Filter out comparisons of the same series \n",
    "            results.append(a.corr(b))  # Apply function\n",
    "\n",
    "result = max(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise 9.3\n",
    "\n",
    "Parallelize the code above with Apache Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Computation time is slower because there is a lot of setup, workers creation, there is a lot of communications the correlation function is too small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise 9.4 Fasta file example\n",
    "\n",
    "We will use this RDD to calculate the frequencies of sequences of five bases, and then sort the sequences into descending order ranked by their frequency.\n",
    "First we will define some functions to split the bases into sequences of a certain size:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Create a rdd from fasta file nucleotide-sample.txt in data directory\n",
    "- Use GroupBy to process every sequence present in the file.\n",
    "- Transform the original text RDD into an RDD containing key-value pairs where the key is the sequence and the value is 1, as per the word-count example.\n",
    "- Notice that if we simply `map` each line of text, we will obtain multi-dimensional data. \n",
    "- Use `flatMap` to flatten this data in order to turn it into a list of base-sequences. \n",
    "- Compute frequencies by using RDD methods `map`, `flatMap`, `reduceByKey`, `sortByKey` and `take`.\n",
    "- Rank each sequence according to its count key, (first element) of each tuple should be the count. Reverse the tuples.\n",
    "- Now we can sort the RDD in descending order of key:"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python (big-data)",
   "language": "python",
   "name": "big-data"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
